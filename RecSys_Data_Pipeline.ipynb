{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import gzip\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class BasePipeline(ABC):\n",
        "    \"\"\"\n",
        "    Abstract base class for dataset pipelines.\n",
        "    Enforces a standard interface for downloading and loading data.\n",
        "    \"\"\"\n",
        "    def __init__(self, save_dir=\"data\"):\n",
        "        self.save_dir = save_dir\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "    def _download_file(self, url):\n",
        "        \"\"\"Helper to download a file with progress indication.\"\"\"\n",
        "        print(f\"Downloading from {url}...\")\n",
        "        try:\n",
        "            response = requests.get(url, stream=True)\n",
        "            response.raise_for_status()\n",
        "            return response.content\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    @abstractmethod\n",
        "    def load_data(self):\n",
        "        \"\"\"Downloads (if necessary) and returns the dataset as a DataFrame.\"\"\"\n",
        "        pass\n",
        "\n",
        "class MovieLensPipeline(BasePipeline):\n",
        "    \"\"\"\n",
        "    Pipeline for the MovieLens Latest Small Dataset.\n",
        "    Standard benchmark for collaborative filtering.\n",
        "    \"\"\"\n",
        "    URL = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
        "\n",
        "    def load_data(self):\n",
        "        print(\"-\" * 50)\n",
        "        print(\"Starting MovieLens Pipeline...\")\n",
        "\n",
        "        content = self._download_file(self.URL)\n",
        "        if not content:\n",
        "            return None, None\n",
        "\n",
        "        print(\"Extracting zip file in memory...\")\n",
        "        with zipfile.ZipFile(io.BytesIO(content)) as z:\n",
        "            # We specifically want ratings.csv and movies.csv\n",
        "            # The zip structure usually has a root folder like 'ml-latest-small/'\n",
        "\n",
        "            # 1. Load Ratings\n",
        "            print(\"Loading ratings...\")\n",
        "            with z.open(\"ml-latest-small/ratings.csv\") as f:\n",
        "                ratings_df = pd.read_csv(f)\n",
        "\n",
        "            # 2. Load Metadata (Movies)\n",
        "            print(\"Loading movie metadata...\")\n",
        "            with z.open(\"ml-latest-small/movies.csv\") as f:\n",
        "                movies_df = pd.read_csv(f)\n",
        "\n",
        "        print(f\"MovieLens Loaded: {len(ratings_df)} ratings, {len(movies_df)} movies.\")\n",
        "        return ratings_df, movies_df\n",
        "\n",
        "class AmazonBeautyPipeline(BasePipeline):\n",
        "    \"\"\"\n",
        "    Pipeline for Amazon Beauty (5-core) Dataset.\n",
        "    '5-core' means all users and items have at least 5 reviews.\n",
        "    Good for testing typically sparse e-commerce data.\n",
        "    \"\"\"\n",
        "    # Switching to the Stanford SNAP mirror (2014 version) which is more stable\n",
        "    # than the UCSD datarepo which frequently throws 404s.\n",
        "    URL = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty_5.json.gz\"\n",
        "\n",
        "    def load_data(self):\n",
        "        print(\"-\" * 50)\n",
        "        print(\"Starting Amazon Beauty Pipeline...\")\n",
        "\n",
        "        # Define local path to cache the large file\n",
        "        local_filename = os.path.join(self.save_dir, \"Beauty_5.json.gz\")\n",
        "\n",
        "        if os.path.exists(local_filename):\n",
        "            print(f\"Found cached file at {local_filename}\")\n",
        "            content = open(local_filename, \"rb\").read()\n",
        "        else:\n",
        "            content = self._download_file(self.URL)\n",
        "            if content:\n",
        "                with open(local_filename, \"wb\") as f:\n",
        "                    f.write(content)\n",
        "\n",
        "        if not content:\n",
        "            return None\n",
        "\n",
        "        print(\"Parsing JSON-lines (this may take a moment)...\")\n",
        "        data = []\n",
        "        # gzip decompression\n",
        "        with gzip.open(io.BytesIO(content), 'rt', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    # Print structure of first record for report\n",
        "                    if i == 0:\n",
        "                        print(f\"\\n[Raw JSON Structure] Sample keys in first record: {list(record.keys())}\")\n",
        "                    data.append(record)\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Standardize columns for easier merging later\n",
        "        # Keeping: reviewerID, asin (item ID), overall (rating), reviewText, unixReviewTime\n",
        "        df = df[['reviewerID', 'asin', 'overall', 'reviewText', 'summary', 'unixReviewTime']]\n",
        "        df.rename(columns={'overall': 'rating', 'unixReviewTime': 'timestamp'}, inplace=True)\n",
        "\n",
        "        print(f\"Amazon Beauty Loaded: {len(df)} reviews.\")\n",
        "        return df\n",
        "\n",
        "def print_dataset_stats(df, name, file_type, user_col, item_col, rating_col, extra_info=None):\n",
        "    \"\"\"\n",
        "    Generates a statistical report for a given dataframe.\n",
        "    Includes file type, variable structure, and pipeline fit.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*20} {name} Dataset Report {'='*20}\")\n",
        "    print(f\"File Source Type:   {file_type}\")\n",
        "\n",
        "    print(f\"\\nData Structure (Variables):\")\n",
        "    print(f\"{'Variable':<15} | {'Type':<10} | {'Sample Value'}\")\n",
        "    print(\"-\" * 45)\n",
        "    for col in df.columns:\n",
        "        # Get a sample value (dropping NAs to be safe)\n",
        "        sample = df[col].dropna().iloc[0] if not df[col].dropna().empty else \"N/A\"\n",
        "        # Truncate long strings for display\n",
        "        sample_str = str(sample)[:20] + \"...\" if len(str(sample)) > 20 else str(sample)\n",
        "        print(f\"{col:<15} | {str(df[col].dtype):<10} | {sample_str}\")\n",
        "\n",
        "    print(f\"\\nRecommender Pipeline Fit:\")\n",
        "    print(f\"  This dataset structures the essential 'User-Item-Interaction' triplets:\")\n",
        "    print(f\"  1. User Entity (Query):     '{user_col}' -> The ID of the customer/user.\")\n",
        "    print(f\"  2. Item Entity (Candidate): '{item_col}' -> The ID of the product/movie.\")\n",
        "    print(f\"  3. Target Label (Signal):   '{rating_col}' -> Explicit feedback (1-5) to train the model.\")\n",
        "\n",
        "    if extra_info:\n",
        "        print(f\"  4. Side Information:        {extra_info}\")\n",
        "        print(\"     (Useful for Hybrid Filtering or cold-start scenarios)\")\n",
        "\n",
        "    n_users = df[user_col].nunique()\n",
        "    n_items = df[item_col].nunique()\n",
        "    n_interactions = len(df)\n",
        "\n",
        "    # Sparsity calculation: 1 - (interactions / (users * items))\n",
        "    # This is critical for RecSys: high sparsity (>99%) often requires\n",
        "    # specific techniques like Matrix Factorization or BPR.\n",
        "    matrix_size = n_users * n_items\n",
        "    sparsity = (1 - (n_interactions / matrix_size)) * 100 if matrix_size > 0 else 0\n",
        "\n",
        "    print(f\"\\nStatistics:\")\n",
        "    print(f\"  Total Interactions: {n_interactions:,}\")\n",
        "    print(f\"  Unique Users:       {n_users:,}\")\n",
        "    print(f\"  Unique Items:       {n_items:,}\")\n",
        "    print(f\"  Matrix Sparsity:    {sparsity:.4f}%\")\n",
        "\n",
        "    print(f\"\\nRating Distribution:\")\n",
        "    print(df[rating_col].describe().round(2))\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "def run_pipelines():\n",
        "    \"\"\"\n",
        "    Orchestrator to run both pipelines.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. MovieLens\n",
        "    ml_pipeline = MovieLensPipeline()\n",
        "    ml_ratings, ml_movies = ml_pipeline.load_data()\n",
        "\n",
        "    if ml_ratings is not None:\n",
        "        print_dataset_stats(\n",
        "            ml_ratings,\n",
        "            \"MovieLens Latest Small\",\n",
        "            \"CSV (Zipped)\",\n",
        "            \"userId\",\n",
        "            \"movieId\",\n",
        "            \"rating\",\n",
        "            extra_info=\"Time-stamp data allows for temporal splitting (Train/Test by time).\"\n",
        "        )\n",
        "\n",
        "    # 2. Amazon\n",
        "    amzn_pipeline = AmazonBeautyPipeline()\n",
        "    amzn_df = amzn_pipeline.load_data()\n",
        "\n",
        "    if amzn_df is not None:\n",
        "        print_dataset_stats(\n",
        "            amzn_df,\n",
        "            \"Amazon Beauty (5-core)\",\n",
        "            \"JSON Lines (Gzipped)\",\n",
        "            \"reviewerID\",\n",
        "            \"asin\",\n",
        "            \"rating\",\n",
        "            extra_info=\"Includes 'reviewText' for NLP and 'timestamp' for sequential/RL modeling.\\n     'asin' is the Amazon Standard Identification Number (Product ID).\"\n",
        "        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        run_pipelines()\n",
        "        print(\"\\nPipeline execution complete. Data is ready for the recommender model.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "Starting MovieLens Pipeline...\n",
            "Downloading from https://files.grouplens.org/datasets/movielens/ml-latest-small.zip...\n",
            "Extracting zip file in memory...\n",
            "Loading ratings...\n",
            "Loading movie metadata...\n",
            "MovieLens Loaded: 100836 ratings, 9742 movies.\n",
            "\n",
            "==================== MovieLens Latest Small Dataset Report ====================\n",
            "File Source Type:   CSV (Zipped)\n",
            "\n",
            "Data Structure (Variables):\n",
            "Variable        | Type       | Sample Value\n",
            "---------------------------------------------\n",
            "userId          | int64      | 1\n",
            "movieId         | int64      | 1\n",
            "rating          | float64    | 4.0\n",
            "timestamp       | int64      | 964982703\n",
            "\n",
            "Recommender Pipeline Fit:\n",
            "  This dataset structures the essential 'User-Item-Interaction' triplets:\n",
            "  1. User Entity (Query):     'userId' -> The ID of the customer/user.\n",
            "  2. Item Entity (Candidate): 'movieId' -> The ID of the product/movie.\n",
            "  3. Target Label (Signal):   'rating' -> Explicit feedback (1-5) to train the model.\n",
            "  4. Side Information:        Time-stamp data allows for temporal splitting (Train/Test by time).\n",
            "     (Useful for Hybrid Filtering or cold-start scenarios)\n",
            "\n",
            "Statistics:\n",
            "  Total Interactions: 100,836\n",
            "  Unique Users:       610\n",
            "  Unique Items:       9,724\n",
            "  Matrix Sparsity:    98.3000%\n",
            "\n",
            "Rating Distribution:\n",
            "count    100836.00\n",
            "mean          3.50\n",
            "std           1.04\n",
            "min           0.50\n",
            "25%           3.00\n",
            "50%           3.50\n",
            "75%           4.00\n",
            "max           5.00\n",
            "Name: rating, dtype: float64\n",
            "============================================================\n",
            "--------------------------------------------------\n",
            "Starting Amazon Beauty Pipeline...\n",
            "Downloading from http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty_5.json.gz...\n",
            "Parsing JSON-lines (this may take a moment)...\n",
            "\n",
            "[Raw JSON Structure] Sample keys in first record: ['reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText', 'overall', 'summary', 'unixReviewTime', 'reviewTime']\n",
            "Amazon Beauty Loaded: 198502 reviews.\n",
            "\n",
            "==================== Amazon Beauty (5-core) Dataset Report ====================\n",
            "File Source Type:   JSON Lines (Gzipped)\n",
            "\n",
            "Data Structure (Variables):\n",
            "Variable        | Type       | Sample Value\n",
            "---------------------------------------------\n",
            "reviewerID      | object     | A1YJEY40YUW4SE\n",
            "asin            | object     | 7806397051\n",
            "rating          | float64    | 1.0\n",
            "reviewText      | object     | Very oily and creamy...\n",
            "summary         | object     | Don't waste your mon...\n",
            "timestamp       | int64      | 1391040000\n",
            "\n",
            "Recommender Pipeline Fit:\n",
            "  This dataset structures the essential 'User-Item-Interaction' triplets:\n",
            "  1. User Entity (Query):     'reviewerID' -> The ID of the customer/user.\n",
            "  2. Item Entity (Candidate): 'asin' -> The ID of the product/movie.\n",
            "  3. Target Label (Signal):   'rating' -> Explicit feedback (1-5) to train the model.\n",
            "  4. Side Information:        Includes 'reviewText' for NLP and 'timestamp' for sequential/RL modeling.\n",
            "     'asin' is the Amazon Standard Identification Number (Product ID).\n",
            "     (Useful for Hybrid Filtering or cold-start scenarios)\n",
            "\n",
            "Statistics:\n",
            "  Total Interactions: 198,502\n",
            "  Unique Users:       22,363\n",
            "  Unique Items:       12,101\n",
            "  Matrix Sparsity:    99.9266%\n",
            "\n",
            "Rating Distribution:\n",
            "count    198502.00\n",
            "mean          4.19\n",
            "std           1.17\n",
            "min           1.00\n",
            "25%           4.00\n",
            "50%           5.00\n",
            "75%           5.00\n",
            "max           5.00\n",
            "Name: rating, dtype: float64\n",
            "============================================================\n",
            "\n",
            "Pipeline execution complete. Data is ready for the recommender model.\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBKnQuGk8Iw4",
        "outputId": "9c2f70b4-ac64-49ba-caa8-2bf0e8081bf9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}